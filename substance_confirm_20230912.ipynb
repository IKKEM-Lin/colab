{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOup7JUFrUi0CkuAkmGwqzb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IKKEM-Lin/colab/blob/main/substance_confirm_20230912.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 反应物质的提取、信息收集（需上传反应json文件）\n",
        "\n"
      ],
      "metadata": {
        "id": "jsQWYSTbjA3R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 导入模块及定义公共函数"
      ],
      "metadata": {
        "id": "ermZfHFjjPhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Levenshtein\n",
        "\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "from pathlib import Path\n",
        "import re\n",
        "from Levenshtein import distance\n",
        "\n",
        "def get_concepts(name):\n",
        "  res = requests.get(f\"https://pubchem.ncbi.nlm.nih.gov/rest/pug/concepts/name/JSON?name={name}\").json()\n",
        "  CIDs = res.get(\"ConceptsAndCIDs\", {}).get(\"CID\", [])\n",
        "  return CIDs and CIDs[0]\n",
        "\n",
        "def get_search_type(name):\n",
        "  if \" \" in name:\n",
        "    return \"text\"\n",
        "  query = {\"query\":{\"type\":\"validity\",\"parameter\":[{\"name\":\"Query\",\"string\":\"{name}\"}]}}\n",
        "  query = json.dumps(query)\n",
        "  res = requests.get(f'https://pubchem.ncbi.nlm.nih.gov/unified_search/structure_search.cgi?format=json&queryblob={query}').json()\n",
        "  validity = res.get(\"response\", {}).get(\"validity\", [])\n",
        "\n",
        "  # print(res)\n",
        "  smiles = [item.get(\"valid\") for item in validity if item.get(\"type\") == \"SMILES\"][0]\n",
        "  smart = [item.get(\"valid\") for item in validity if item.get(\"type\") == \"SMARTS\"][0]\n",
        "  formula = [item.get(\"valid\") for item in validity if item.get(\"type\") == \"Formula\"][0]\n",
        "  cache = [item.get(\"valid\") for item in validity if item.get(\"type\") == \"Cache\"][0]\n",
        "\n",
        "  if not formula:\n",
        "    return \"text\"\n",
        "  if len(name) < 4:\n",
        "    return \"formular\"\n",
        "\n",
        "  return not(smiles or smart or cache) and \"formular\" or \"text\"\n",
        "\n",
        "def search_by_formular(name):\n",
        "  query = {\"query\":{\"type\":\"formula\",\"parameter\":[{\"name\":\"FormulaQuery\",\"string\": str(name)},{\"name\":\"UseCache\",\"bool\":False},{\"name\":\"SearchTimeMsec\",\"num\":5000},{\"name\":\"SearchMaxRecords\",\"num\":10},{\"name\":\"allowotherelements\",\"bool\":False}]}}\n",
        "  query = json.dumps(query)\n",
        "  res = requests.get(f'https://pubchem.ncbi.nlm.nih.gov/unified_search/structure_search.cgi?format=json&queryblob={query}').json()\n",
        "  hitcount = res.get(\"response\", {}).get(\"hitcount\", 0)\n",
        "  if hitcount > 0:\n",
        "    try:\n",
        "      column = res.get(\"response\", {}).get(\"table\", {}).get(\"column\", [])\n",
        "      print(column, )\n",
        "      cids = list(filter(lambda x: x.get(\"name\") == \"CID\", column))[0].get(\"num\", [])\n",
        "      return cids\n",
        "    except:\n",
        "      pass\n",
        "  return []\n",
        "\n",
        "def search_by_text(name):\n",
        "  names = name.split(\" \")\n",
        "  query = {\"select\":\"*\",\"collection\":\"compound\",\"where\":{\"ands\":[{\"*\":item} for item in names]},\"order\":[\"relevancescore,desc\"],\"start\":1,\"limit\":10,\"width\":1000000,\"listids\":0}\n",
        "  query = json.dumps(query)\n",
        "  res = requests.get(f'https://pubchem.ncbi.nlm.nih.gov/sdq/sdqagent.cgi?infmt=json&outfmt=json&query={query}').json()\n",
        "  compounds = [item.get(\"rows\") for item in res.get(\"SDQOutputSet\", []) if item.get(\"collection\") == \"compound\"]\n",
        "  return compounds and compounds[0]\n",
        "\n",
        "def get_cid_detail(cid):\n",
        "  res = requests.get(f\"https://pubchem.ncbi.nlm.nih.gov/rest/pug_view/data/compound/{cid}/JSON/\").json()\n",
        "  return res"
      ],
      "metadata": {
        "id": "RxGW-N5Z887c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 抽取所有物质及其频次"
      ],
      "metadata": {
        "id": "F9EDGuuMjX2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_reaction_file = \"all_reaction.json\"\n",
        "all_substance_file = \"all_uniq_substance.json\"\n",
        "substance_freqency_file = \"freqency.json\"\n",
        "\n",
        "all_substance = []\n",
        "all_uniq_substance = []\n",
        "substance_frequency = {}\n",
        "\n",
        "def add_substance_from_reaction(reaction):\n",
        "  def add_substance(substance_list):\n",
        "    for item in substance_list:\n",
        "      substance = isinstance(item, dict) and (item.get(\"name\") or item.get(\"mf\"))\n",
        "      if not substance:\n",
        "        continue\n",
        "      if not isinstance(substance, str):\n",
        "        continue\n",
        "      all_substance.append(substance.strip())\n",
        "  reactants = reaction.get(\"data\", {}).get(\"reactants\")\n",
        "  catalysts = reaction.get(\"data\", {}).get(\"catalysts\")\n",
        "  balance_gases = reaction.get(\"data\", {}).get(\"balance_gases\")\n",
        "  solvents = reaction.get(\"data\", {}).get(\"solvents\")\n",
        "  intermediates = reaction.get(\"data\", {}).get(\"intermediates\")\n",
        "  products = reaction.get(\"data\", {}).get(\"products\")\n",
        "\n",
        "  isinstance(reactants, list) and add_substance(reactants)\n",
        "  isinstance(catalysts, list) and add_substance(catalysts)\n",
        "  isinstance(solvents, list) and add_substance(solvents)\n",
        "  isinstance(intermediates, list) and add_substance(intermediates)\n",
        "  isinstance(products, list) and add_substance(products)\n",
        "\n",
        "\n",
        "with open(all_reaction_file, \"rb\") as f:\n",
        "  reactions = json.load(f)\n",
        "  for reaction in reactions:\n",
        "    add_substance_from_reaction(reaction)\n",
        "  all_uniq_substance = list(set(all_substance))\n",
        "  print(len(all_uniq_substance))\n",
        "  for ind, uniq_substance in enumerate(all_uniq_substance):\n",
        "    print(f\"start {ind}, {ind / len(all_uniq_substance)}\")\n",
        "    substance_frequency[uniq_substance] = all_substance.count(uniq_substance)\n",
        "\n",
        "  with open(all_substance_file, \"w\") as fs:\n",
        "    fs.write(json.dumps(all_uniq_substance))\n",
        "  with open(substance_freqency_file, \"w\") as fs:\n",
        "    fs.write(json.dumps(substance_frequency))\n"
      ],
      "metadata": {
        "id": "5VYbod-AScJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. 初步筛选出可获取cid的物质"
      ],
      "metadata": {
        "id": "IISSP83dje1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cid_match_file = \"cid_match.jsonl\"\n",
        "\n",
        "with open(all_substance_file, \"rb\") as f:\n",
        "  content = json.load(f)\n",
        "\n",
        "cache_matched = []\n",
        "final_file = Path(cid_match_file)\n",
        "if final_file.is_file():\n",
        "  with open(cid_match_file, \"r\") as f:\n",
        "    for line in f:\n",
        "      result = json.loads(line)\n",
        "      result.get(\"id\") and cache_matched.append(result.get(\"id\"))\n",
        "else:\n",
        "  with open(cid_match_file, \"w\") as f:\n",
        "    pass\n",
        "\n",
        "\n",
        "\n",
        "for ind, substance in enumerate(content):\n",
        "  if substance in cache_matched:\n",
        "    continue\n",
        "  print(f\"Start working on ${ind} substance, {substance}\")\n",
        "  cid = get_concepts(substance)\n",
        "  search_type = \"\"\n",
        "  if not cid:\n",
        "    search_type = get_search_type(substance)\n",
        "    if search_type == \"formular\":\n",
        "      res = search_by_formular(substance)\n",
        "      cid = res and res[0] or \"\"\n",
        "    else:\n",
        "      res = search_by_text(substance)\n",
        "      cid = res and res[0].get(\"cid\") or \"\"\n",
        "\n",
        "  with open(cid_match_file, \"a\", encoding=\"utf-8\") as f:\n",
        "    print(f'{{\"index\": \"{ind}\", \"id\": \"{substance}\", \"cid\": \"{cid}\", \"search_type\": \"{search_type}\"  }}', file=f)\n",
        "    cache_matched.append(substance)\n",
        "  time.sleep(1)\n"
      ],
      "metadata": {
        "id": "l_2Z1aQh9z4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. 获取最终的信息及计算文本相似度"
      ],
      "metadata": {
        "id": "jykFET7Zjrmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_file = \"final_result.jsonl\"\n",
        "\n",
        "with open(substance_freqency_file, \"r\", encoding=\"utf-8\") as f:\n",
        "  substance_freqency = json.load(f)\n",
        "\n",
        "main_matched = {}\n",
        "final_file = Path(result_file)\n",
        "if final_file.is_file():\n",
        "  with open(result_file, \"r\") as f:\n",
        "    for line in f:\n",
        "      result = json.loads(line)\n",
        "      main_matched[result.get(\"name\")] = result\n",
        "else:\n",
        "  with open(result_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    pass\n",
        "\n",
        "\n",
        "def main():\n",
        "  substance_with_cid = []\n",
        "  with open(cid_match_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "      item = json.loads(line)\n",
        "      substance_with_cid.append({\"id\": item.get(\"id\"), \"cid\": item.get(\"cid\"), \"search_type\": item.get(\"search_type\")})\n",
        "\n",
        "  substance_with_cid_len = len(substance_with_cid)\n",
        "  for inde, item in enumerate(substance_with_cid):\n",
        "    print(f'Start working on ${inde}, ${item.get(\"id\")}, {inde / substance_with_cid_len}')\n",
        "    cid = item.get(\"cid\")\n",
        "    if not cid:\n",
        "      continue\n",
        "    search_type = item.get(\"search_type\", \"\")\n",
        "    name = item.get(\"id\")\n",
        "    freq = substance_freqency.get(name)\n",
        "\n",
        "    if main_matched.get(name):\n",
        "      continue\n",
        "\n",
        "    res = get_cid_detail(cid)\n",
        "\n",
        "    if not res:\n",
        "      print(f\"Error: {cid}\")\n",
        "      continue\n",
        "\n",
        "    names_identifiers = res.get(\"Record\", {}).get(\"Section\", [])\n",
        "    try:\n",
        "      names_identifiers = list(filter(lambda x: x.get(\"TOCHeading\") == \"Names and Identifiers\", names_identifiers))[0].get(\"Section\", [])\n",
        "      names_identifiers = list(filter(lambda x: x.get(\"TOCHeading\") == \"Synonyms\", names_identifiers))[0].get(\"Section\", [])\n",
        "      names_identifiers = list(filter(lambda x: x.get(\"TOCHeading\") == \"Depositor-Supplied Synonyms\", names_identifiers))[0].get(\"Information\", [])[0].get(\"Value\", \"\").get(\"StringWithMarkup\", [])\n",
        "      names_identifiers = list(map(lambda x: x.get(\"String\"), names_identifiers))\n",
        "    except:\n",
        "      names_identifiers = []\n",
        "\n",
        "    cas_id = ''\n",
        "    chebi_id = ''\n",
        "    edit_distance = []\n",
        "    for item in names_identifiers:\n",
        "      # if cas_id and chebi_id:\n",
        "      #   break\n",
        "      edit_distance.append(distance(name, item))\n",
        "      if not cas_id and re.match(r\"^\\d{2,7}-\\d{2}-\\d$\", item):\n",
        "        cas_id = item\n",
        "      if not chebi_id and  re.match(r\"^CHEBI:\\d+$\", item):\n",
        "        chebi_id = item\n",
        "    if edit_distance:\n",
        "      min_edit_distance = min(edit_distance)\n",
        "      min_edit_distance_index = edit_distance.index(min_edit_distance)\n",
        "      diff_name = names_identifiers[min_edit_distance_index]\n",
        "    with open(result_file, \"a\", encoding=\"utf-8\") as f:\n",
        "      print(f'{{\"cid\":\"{cid}\", \"name\":\"{name}\", \"diff_name\":\"{diff_name}\", \"edit_d\":\"{min_edit_distance}\", \"cas\":\"{cas_id}\", \"chebi\":\"{chebi_id}\", \"freq\": \"{freq}\", \"type\":\"{search_type}\"}}', file=f)\n",
        "    main_matched[cid] = json.loads(f'{{\"cid\":\"{cid}\", \"name\":\"{name}\", \"diff_name\":\"{diff_name}\", \"edit_d\":\"{min_edit_distance}\", \"cas\":\"{cas_id}\", \"chebi\":\"{chebi_id}\", \"freq\": \"{freq}\", \"type\":\"{search_type}\"}}')\n",
        "    time.sleep(1)\n",
        "main()"
      ],
      "metadata": {
        "id": "IKvbV6QmDF9H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}